{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is useful to find the best parameters for the RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.preprocessing as sk\n",
    "\n",
    "import helper_functions as hf\n",
    "\n",
    "from analysis_tools import *\n",
    "\n",
    "CURDIR = os.getcwd()\n",
    "DATADIR = os.path.join(CURDIR,  \"data\")\n",
    "FIGDIR = os.path.join(CURDIR,  \"figure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list_of_parameters = {\n",
    "'type_of_data' : ['Price', 'All'],\n",
    "'add_return' : [False, True],\n",
    "'add_log_functions' : [False,True],\n",
    "'drop_higly_correlated_data' : [True, False],\n",
    "'add_analysed_curves' : [True, False],\n",
    "'add_return' : [True, False],\n",
    "'scaler' : [sk.MinMaxScaler],\n",
    "'feature_range' :  [(0,1), (-1,1)],\n",
    "\n",
    "'rnn_size' : [10,1,7, 8, 9, 11, 12],\n",
    "'size_train' : [2000],\n",
    "'size_valid' : [800],\n",
    "'target_id' : [0],\n",
    "'type_of_rnn' : ['GRU', 'LSTM'],\n",
    "'nb_layers' : [1],\n",
    "'nb_units' : [[128],[256]],\n",
    "'learning_rate' : [0.001],\n",
    "'optimizer' : [keras.optimizers.Adam],\n",
    "'loss' : [keras.losses.MeanSquaredError()],\n",
    "'batch_size' : [4,8,16],\n",
    "'epochs' : [120]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'type_of_data' : 'Price',\n",
    "    'add_log_functions' : True,\n",
    "    'drop_higly_correlated_data' : False,\n",
    "    'add_analysed_curves' :False,\n",
    "    'add_return' : False,\n",
    "    'scaler' :sk.MinMaxScaler,\n",
    "    'feature_range': (0,1),\n",
    "\n",
    "    'rnn_size' :1,\n",
    "    'size_train' : 2000,\n",
    "    'size_valid' : 800,\n",
    "    'target_id' : 0,\n",
    "    'type_of_rnn' : 'GRU',\n",
    "    'nb_layers' : 1,\n",
    "    'nb_units' : [128],\n",
    "    'learning_rate' : 0.001,\n",
    "    'optimizer' : keras.optimizers.Adam,\n",
    "    'loss' : keras.losses.MeanSquaredError(),\n",
    "    'batch_size' : 16,\n",
    "    'epochs' : 180\n",
    "\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params): # takes as agument the parameters of the model and returns the rmse of this model\n",
    "    \n",
    "    type_of_data = params['type_of_data']\n",
    "    add_log_functions = params['add_log_functions']\n",
    "    drop_higly_correlated_data = params['drop_higly_correlated_data']\n",
    "    add_analysed_curves = params['add_analysed_curves']\n",
    "    add_return = params['add_return']\n",
    "    scaler, feature_range = params['scaler'], params['feature_range']\n",
    "\n",
    "    # params model rnn\n",
    "    rnn_size = params['rnn_size']\n",
    "    size_train = params['size_train']\n",
    "    size_valid = params['size_valid']\n",
    "    size_test = 3544 - size_train - size_valid\n",
    "    target_id = params['target_id']\n",
    "    type_of_rnn = params['type_of_rnn']\n",
    "    nb_layers = params['nb_layers']\n",
    "    nb_units = params['nb_units']\n",
    "    learning_rate = params['learning_rate']\n",
    "    optimizer = params['optimizer']\n",
    "    loss = params['loss']\n",
    "    batch_size = params['batch_size']\n",
    "    epochs = params['epochs']\n",
    "\n",
    "    df_blockchain = pd.read_csv(os.path.join(DATADIR, \"df_blockchain.csv\"), \n",
    "                            delimiter=\",\")\n",
    "    # nettoyer les datas\n",
    "    index_with_nan = df_blockchain.index[df_blockchain.isnull().any(axis=1)]\n",
    "    df_blockchain.drop(index_with_nan,0, inplace=True)\n",
    "    if add_log_functions:\n",
    "        df_blockchain['market-price'] = np.log(df_blockchain['market-price'])\n",
    "    if type_of_data == 'Price':\n",
    "        if add_analysed_curves:\n",
    "            df = df_blockchain[\"market-price\"]\n",
    "            ma1_2 = deltamma(df, 20, 50)\n",
    "            macd1, macd2, macd3 = macd(df, .75, 12, 26)\n",
    "            boll = bollinger(df, 20, 2)\n",
    "            new_val = []\n",
    "            for i in [ma1_2, macd1, macd3, boll]:\n",
    "                for j in i.columns:\n",
    "                    new_val.append(j)\n",
    "            macd1 = macd1[50 - 26:]\n",
    "            macd2 = macd2[50 - 26 - 9:]\n",
    "            macd3 = macd3[50 - 26 - 9:]\n",
    "            boll = boll[50 - 20:]\n",
    "            df_blockchain = df_blockchain[50:]\n",
    "            macd1.reset_index(inplace=True)\n",
    "            macd2.reset_index(inplace=True)\n",
    "            macd3.reset_index(inplace=True)\n",
    "            boll.reset_index(inplace=True)\n",
    "            df_blockchain.reset_index(inplace=True)\n",
    "            df_blockchain = pd.concat([df_blockchain, ma1_2, macd1, macd3, boll], axis=1)\n",
    "\n",
    "            columns = [\"market-price\"]+new_val\n",
    "            dataset = df_blockchain[columns]\n",
    "        else:\n",
    "            columns = ['market-price']\n",
    "            dataset = df_blockchain[columns]\n",
    "\n",
    "    if type_of_data == 'All':\n",
    "        if add_analysed_curves:\n",
    "            df = df_blockchain['market-price']\n",
    "            ma1_2 = deltamma(df, 20, 50)\n",
    "            macd1, macd2, macd3 = macd(df, .75, 12, 26)\n",
    "            boll = bollinger(df, 20, 2)\n",
    "            new_val = []\n",
    "            for i in [ma1_2, macd1, macd3, boll]:\n",
    "                for j in i.columns:\n",
    "                    new_val.append(j)\n",
    "            macd1 = macd1[50 - 26:]\n",
    "            macd2 = macd2[50 - 26 - 9:]\n",
    "            macd3 = macd3[50 - 26 - 9:]\n",
    "            boll = boll[50 - 20:]\n",
    "            df_blockchain = df_blockchain[50:]\n",
    "            macd1.reset_index(inplace=True)\n",
    "            macd2.reset_index(inplace=True)\n",
    "            macd3.reset_index(inplace=True)\n",
    "            boll.reset_index(inplace=True)\n",
    "            df_blockchain.reset_index(inplace=True)\n",
    "            df_blockchain = pd.concat([df_blockchain, ma1_2, macd1, macd3, boll], axis=1)\n",
    "\n",
    "            if drop_higly_correlated_data:\n",
    "                columns = [\"market-price\",\"n-transactions-per-block\",\"hash-rate\",\"difficulty\",\"miners-revenue\",\"trade-volume\",\"blocks-size\",\"avg-block-size\",\"transaction-fees\",\"transaction-fees-usd\",\"cost-per-transaction-percent\",\"cost-per-transaction\",\"n-transactions\",\"n-transactions-total\",\"n-transactions-excluding-popular\",\"estimated-transaction-volume-usd\",\"total-bitcoins\",\"market-cap\"]\n",
    "                columns = columns + new_val\n",
    "                dataset = df_blockchain[columns]   \n",
    "            else:\n",
    "                columns = list(df_blockchain.columns)[1:] + new_val\n",
    "                dataset = df_blockchain[columns]\n",
    "        else:\n",
    "            if drop_higly_correlated_data:\n",
    "                columns = [\"market-price\",\"n-transactions-per-block\",\"hash-rate\",\"difficulty\",\"miners-revenue\",\"trade-volume\",\"blocks-size\",\"avg-block-size\",\"transaction-fees\",\"transaction-fees-usd\",\"cost-per-transaction-percent\",\"cost-per-transaction\",\"n-transactions\",\"n-transactions-total\",\"n-transactions-excluding-popular\",\"estimated-transaction-volume-usd\",\"total-bitcoins\",\"market-cap\"]\n",
    "                dataset = df_blockchain[columns]   \n",
    "            else:\n",
    "                columns = list(df_blockchain.columns)[1:]\n",
    "                dataset = df_blockchain[columns]\n",
    "    if add_return:\n",
    "        col = dataset['market-price'].diff().iloc[1:]\n",
    "        col[0] = 0\n",
    "        dataset['ret'] = col\n",
    "        columns.append('ret')\n",
    "\n",
    "    try:\n",
    "        dataset.drop(columns='index', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        dataset.drop(columns='Date', inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    for i,x in enumerate(columns):\n",
    "        if x == 'index' or x == 'Date':\n",
    "            columns.pop(i)\n",
    "\n",
    "    # index_with_nan = dataset.index[dataset.iloc[:,1].isnull()]\n",
    "    # dataset.drop(index_with_nan,0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    scaler = scaler(feature_range=feature_range)\n",
    "    dataset = scaler.fit_transform(dataset.values.reshape(-1,len(columns)))\n",
    "    data_train = dataset[:size_train]\n",
    "    data_valid = dataset[size_train:size_train+size_valid]\n",
    "    data_test = dataset[size_train+size_valid:size_train+size_valid+size_test]\n",
    "    def process_data(data, rnn_size=rnn_size, target_id=target_id, columns_size=len(columns)):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(data)-rnn_size):\n",
    "            X.append(data[i:i+rnn_size,:])\n",
    "            y.append(data[i+rnn_size,target_id])\n",
    "        return np.array(X).astype(np.float32).reshape((-1,rnn_size,columns_size)), np.array(y).astype(np.float32)\n",
    "\n",
    "    # process data for RNN\n",
    "    X_train, y_train = process_data(data_train, rnn_size)\n",
    "    X_val, y_val = process_data(data_valid, rnn_size)\n",
    "    X_test, y_test = process_data(data_test, rnn_size)\n",
    "\n",
    "    callback= keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0,\n",
    "    patience=15,\n",
    "    verbose=0,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights = True)\n",
    "\n",
    "    if type_of_rnn == 'LSTM':\n",
    "        regressor = keras.Sequential()\n",
    "        regressor.add(\n",
    "            keras.layers.Bidirectional(\n",
    "                keras.layers.LSTM(\n",
    "                    units = nb_units[0],\n",
    "                    input_shape = (X_train.shape[1],X_train.shape[2] )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for i in range(1,nb_layers):\n",
    "            regressor.add(keras.layers.Dense(units = nb_units[i]))\n",
    "        regressor.add(keras.layers.Dense(units = 1))\n",
    "        \n",
    "        opt = optimizer(learning_rate=learning_rate)\n",
    "        #Compiling the Recurrent Neural Network with adam optimier and 'mean_absolute_error' as loss function\n",
    "        regressor.compile(loss=loss, optimizer=opt)\n",
    "    else:\n",
    "        regressor = keras.Sequential()\n",
    "        regressor.add(\n",
    "            keras.layers.Bidirectional(\n",
    "                keras.layers.GRU(\n",
    "                    units = nb_units[0],\n",
    "                    input_shape = (X_train.shape[1],X_train.shape[2] )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        for i in range(1,nb_layers):\n",
    "            regressor.add(keras.layers.Dense(units = nb_units[i]))\n",
    "        regressor.add(keras.layers.Dense(units = 1))\n",
    "        \n",
    "        opt = optimizer(learning_rate=learning_rate)\n",
    "        #Compiling the Recurrent Neural Network with adam optimier and 'mean_absolute_error' as loss function\n",
    "        regressor.compile(loss=loss, optimizer=opt)\n",
    "    history = regressor.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = batch_size, epochs = epochs, callbacks=[callback])\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    # compute rmse for test\n",
    "    y_pred_inverse = scaler.inverse_transform(np.concatenate([y_pred, data_test[-len(y_pred):,1:]], axis=1))\n",
    "    y_test_inverse = scaler.inverse_transform(data_test.reshape(-1,len(columns)))[rnn_size:]\n",
    "    if add_log_functions:\n",
    "        y_pred_inverse = np.exp(y_pred_inverse)\n",
    "        y_test_inverse = np.exp(y_test_inverse)\n",
    "    rmse_score = np.sqrt(np.square(np.subtract(y_pred_inverse, y_test_inverse)).mean())\n",
    "    return rmse_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode par tatonnement, on cherche les meilleurs paramètres de façon 'indépendante'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait varier le rnn size\n",
    "\n",
    "# values = []\n",
    "\n",
    "# for size in range(1,20):\n",
    "#     print(size)\n",
    "#     params['rnn_size'] = size\n",
    "#     tot = 0\n",
    "#     nb_test = 10\n",
    "#     for j in range(nb_test):\n",
    "#         tot+=evaluate(params)\n",
    "#     values.append(tot/nb_test)\n",
    "    \n",
    "# min en 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On fait varier le nombre de couches\n",
    "\n",
    "# values = []\n",
    "# for i in [1,2]:\n",
    "#     tot = 0\n",
    "#     params['nb_layers']= i\n",
    "#     if i == 1:\n",
    "#         params['nb_units'] = [128]\n",
    "#     else:\n",
    "#         params['nb_units'] = [128, 64]\n",
    "#     n_test = 5\n",
    "#     for j in range(n_test):\n",
    "#         print('J en suis a ('+str(i)+','+str(j)+')')\n",
    "#         tot+= evaluate(params)\n",
    "#     values.append(tot/n_test)\n",
    "\n",
    "# # on trouve un meilleur résultat avec une seule couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #on utilise le log price ou non \n",
    "\n",
    "# values = []\n",
    "# for bo in [True,False]:\n",
    "#     params['add_log_functions'] = bo\n",
    "#     tot  =0\n",
    "#     n_test = 5\n",
    "#     for j in range(n_test):\n",
    "#         print('J en suis a ('+str(bo)+','+str(j)+')')\n",
    "\n",
    "#         tot+= evaluate(params)\n",
    "#     values.append(tot/n_test)\n",
    "# print(values)\n",
    "\n",
    "# #C'est mieux de ne pas passer au log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # on change le nombre de neurones \n",
    "\n",
    "# values = []\n",
    "# for nb in [64, 128,256]:\n",
    "#     params['nb_units'] = [nb]\n",
    "#     tot  =0\n",
    "#     n_test = 5\n",
    "#     for j in range(n_test):\n",
    "#         print('J en suis a ('+str(nb)+','+str(j)+')')\n",
    "\n",
    "#         tot+= evaluate(params)\n",
    "#     values.append(tot/n_test)\n",
    "# print(values)\n",
    "\n",
    "# valeurs très similaires mais batch size de 128 légèrement meilleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # on fait varier la taille du batch size \n",
    "\n",
    "# values = []\n",
    "# for nb in [2,4]:\n",
    "#     params['batch_size'] = nb\n",
    "#     tot  =0\n",
    "#     n_test = 5\n",
    "#     for j in range(n_test):\n",
    "#         print('J en suis a ('+str(nb)+','+str(j)+')')\n",
    "\n",
    "#         tot+= evaluate(params)\n",
    "#     values.append(tot/n_test)\n",
    "# print(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choix aléatoire de paramètres pour trouver un minimum global de la rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_params_chosen = []\n",
    "N = 2 # Nombre d'ensembles de paramètres \n",
    "for i in range(N):\n",
    "    set = {}\n",
    "    for par in list_of_parameters:\n",
    "        entier = np.random.randint(0,len(list_of_parameters[par]))\n",
    "        x = list_of_parameters[par][entier]\n",
    "        params[par] = x\n",
    "        set[par] = x\n",
    "    liste_params_chosen.append(set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-82442111bbda>:29: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_blockchain.drop(index_with_nan,0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 5s 7ms/step - loss: 0.0156 - val_loss: 0.0019\n",
      "Epoch 2/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 6.9453e-05 - val_loss: 3.8606e-05\n",
      "Epoch 3/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0258e-05 - val_loss: 3.4289e-05\n",
      "Epoch 4/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0311e-05 - val_loss: 3.0738e-05\n",
      "Epoch 5/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0663e-05 - val_loss: 2.1858e-05\n",
      "Epoch 6/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.1167e-05 - val_loss: 2.1447e-05\n",
      "Epoch 7/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0750e-05 - val_loss: 2.3549e-05\n",
      "Epoch 8/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.1666e-05 - val_loss: 5.5303e-05\n",
      "Epoch 9/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0931e-05 - val_loss: 2.7667e-05\n",
      "Epoch 10/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.0980e-05 - val_loss: 2.0485e-05\n",
      "Epoch 11/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2313e-05 - val_loss: 2.2579e-05\n",
      "Epoch 12/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.1564e-05 - val_loss: 3.3478e-05\n",
      "Epoch 13/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.1255e-05 - val_loss: 3.5853e-05\n",
      "Epoch 14/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2174e-05 - val_loss: 2.2103e-05\n",
      "Epoch 15/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.3311e-05 - val_loss: 2.5302e-05\n",
      "Epoch 16/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.3102e-05 - val_loss: 3.7749e-05\n",
      "Epoch 17/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.1863e-05 - val_loss: 4.1646e-05\n",
      "Epoch 18/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2859e-05 - val_loss: 2.0171e-05\n",
      "Epoch 19/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2512e-05 - val_loss: 2.0293e-05\n",
      "Epoch 20/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.4088e-05 - val_loss: 2.0280e-05\n",
      "Epoch 21/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.8139e-05 - val_loss: 4.4261e-05\n",
      "Epoch 22/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.5220e-05 - val_loss: 4.3348e-05\n",
      "Epoch 23/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2385e-05 - val_loss: 2.1020e-05\n",
      "Epoch 24/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.7477e-05 - val_loss: 2.0337e-05\n",
      "Epoch 25/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.3005e-05 - val_loss: 2.3837e-05\n",
      "Epoch 26/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2592e-05 - val_loss: 2.3418e-05\n",
      "Epoch 27/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.4958e-05 - val_loss: 2.0016e-05\n",
      "Epoch 28/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.4250e-05 - val_loss: 2.8993e-05\n",
      "Epoch 29/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2359e-05 - val_loss: 5.1104e-05\n",
      "Epoch 30/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.4766e-05 - val_loss: 2.1022e-05\n",
      "Epoch 31/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.4659e-05 - val_loss: 2.1861e-05\n",
      "Epoch 32/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.7257e-05 - val_loss: 1.9850e-05\n",
      "Epoch 33/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.2904e-05 - val_loss: 2.3507e-05\n",
      "Epoch 34/120\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2.5268e-05 - val_loss: 2.8246e-05\n",
      "Epoch 35/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.2236e-05 - val_loss: 2.4683e-05\n",
      "Epoch 36/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.4800e-05 - val_loss: 2.7821e-05\n",
      "Epoch 37/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.4835e-05 - val_loss: 2.1505e-05\n",
      "Epoch 38/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.4615e-05 - val_loss: 2.3600e-05\n",
      "Epoch 39/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.4251e-05 - val_loss: 2.0100e-05\n",
      "Epoch 40/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.6693e-05 - val_loss: 5.4793e-05\n",
      "Epoch 41/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 3.1239e-05 - val_loss: 1.0317e-04\n",
      "Epoch 42/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.3547e-05 - val_loss: 7.5490e-05\n",
      "Epoch 43/120\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2.5877e-05 - val_loss: 5.0425e-05\n",
      "Epoch 44/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 3.5631e-05 - val_loss: 2.2626e-05\n",
      "Epoch 45/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.3819e-05 - val_loss: 2.1073e-05\n",
      "Epoch 46/120\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2.2405e-05 - val_loss: 4.9581e-05\n",
      "Epoch 47/120\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2.4072e-05 - val_loss: 3.8422e-05\n",
      "23/23 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-82442111bbda>:29: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  df_blockchain.drop(index_with_nan,0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "125/125 [==============================] - 5s 17ms/step - loss: 8.4010e-04 - val_loss: 0.0018\n",
      "Epoch 2/120\n",
      "125/125 [==============================] - 1s 11ms/step - loss: 2.4243e-06 - val_loss: 4.4394e-04\n",
      "Epoch 3/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 1.3205e-06 - val_loss: 4.4131e-04\n",
      "Epoch 4/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 1.2836e-06 - val_loss: 4.1160e-04\n",
      "Epoch 5/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.2654e-06 - val_loss: 3.9135e-04\n",
      "Epoch 6/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.1577e-06 - val_loss: 3.7195e-04\n",
      "Epoch 7/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.0645e-06 - val_loss: 3.5468e-04\n",
      "Epoch 8/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 1.1498e-06 - val_loss: 3.3477e-04\n",
      "Epoch 9/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.0884e-06 - val_loss: 3.1281e-04\n",
      "Epoch 10/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 9.0473e-07 - val_loss: 2.9355e-04\n",
      "Epoch 11/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.2239e-06 - val_loss: 2.7847e-04\n",
      "Epoch 12/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.6555e-06 - val_loss: 2.7133e-04\n",
      "Epoch 13/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.3405e-06 - val_loss: 2.4772e-04\n",
      "Epoch 14/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.0341e-06 - val_loss: 2.2989e-04\n",
      "Epoch 15/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.3266e-06 - val_loss: 2.2661e-04\n",
      "Epoch 16/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.5039e-06 - val_loss: 2.1385e-04\n",
      "Epoch 17/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 1.6958e-06 - val_loss: 1.9478e-04\n",
      "Epoch 18/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.5309e-06 - val_loss: 1.7669e-04\n",
      "Epoch 19/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 9.6244e-07 - val_loss: 1.8588e-04\n",
      "Epoch 20/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5887e-06 - val_loss: 1.6744e-04\n",
      "Epoch 21/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 3.1217e-06 - val_loss: 1.4919e-04\n",
      "Epoch 22/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.2435e-06 - val_loss: 1.4636e-04\n",
      "Epoch 23/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.7487e-06 - val_loss: 1.3286e-04\n",
      "Epoch 24/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.3524e-06 - val_loss: 1.4039e-04\n",
      "Epoch 25/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.6729e-06 - val_loss: 1.2452e-04\n",
      "Epoch 26/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 3.4383e-06 - val_loss: 1.3898e-04\n",
      "Epoch 27/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.1619e-06 - val_loss: 1.2387e-04\n",
      "Epoch 28/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.9303e-06 - val_loss: 1.1916e-04\n",
      "Epoch 29/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.6749e-06 - val_loss: 1.0656e-04\n",
      "Epoch 30/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 3.9312e-06 - val_loss: 1.1607e-04\n",
      "Epoch 31/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.9432e-06 - val_loss: 1.3140e-04\n",
      "Epoch 32/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 3.2754e-06 - val_loss: 9.6318e-05\n",
      "Epoch 33/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.8078e-06 - val_loss: 9.6493e-05\n",
      "Epoch 34/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 3.2329e-06 - val_loss: 8.8446e-05\n",
      "Epoch 35/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.9007e-06 - val_loss: 8.5199e-05\n",
      "Epoch 36/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.9640e-06 - val_loss: 8.6873e-05\n",
      "Epoch 37/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 3.3891e-06 - val_loss: 8.7497e-05\n",
      "Epoch 38/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 2.1824e-06 - val_loss: 9.8019e-05\n",
      "Epoch 39/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.6279e-06 - val_loss: 8.1254e-05\n",
      "Epoch 40/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.0441e-06 - val_loss: 1.1550e-04\n",
      "Epoch 41/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.9792e-06 - val_loss: 8.2754e-05\n",
      "Epoch 42/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5289e-06 - val_loss: 7.4265e-05\n",
      "Epoch 43/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5025e-06 - val_loss: 1.0040e-04\n",
      "Epoch 44/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.8295e-06 - val_loss: 7.9154e-05\n",
      "Epoch 45/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.3669e-06 - val_loss: 9.1378e-05\n",
      "Epoch 46/120\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2.6787e-06 - val_loss: 6.8223e-05\n",
      "Epoch 47/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.3924e-06 - val_loss: 6.7424e-05\n",
      "Epoch 48/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.5196e-06 - val_loss: 7.2873e-05\n",
      "Epoch 49/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.0893e-06 - val_loss: 6.3992e-05\n",
      "Epoch 50/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.6326e-06 - val_loss: 6.2709e-05\n",
      "Epoch 51/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.6426e-06 - val_loss: 6.2318e-05\n",
      "Epoch 52/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 2.1184e-06 - val_loss: 7.1324e-05\n",
      "Epoch 53/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 1.5587e-06 - val_loss: 6.0784e-05\n",
      "Epoch 54/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.4426e-06 - val_loss: 6.2953e-05\n",
      "Epoch 55/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 7.2579e-07 - val_loss: 5.7540e-05\n",
      "Epoch 56/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.7552e-06 - val_loss: 7.0383e-05\n",
      "Epoch 57/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.6446e-06 - val_loss: 6.6581e-05\n",
      "Epoch 58/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.7184e-06 - val_loss: 6.0601e-05\n",
      "Epoch 59/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.4124e-06 - val_loss: 5.9356e-05\n",
      "Epoch 60/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.0259e-06 - val_loss: 6.2288e-05\n",
      "Epoch 61/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.1487e-06 - val_loss: 5.7572e-05\n",
      "Epoch 62/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 9.6632e-07 - val_loss: 6.3593e-05\n",
      "Epoch 63/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.1341e-06 - val_loss: 5.4310e-05\n",
      "Epoch 64/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.8056e-06 - val_loss: 7.6037e-05\n",
      "Epoch 65/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 8.9236e-07 - val_loss: 5.5313e-05\n",
      "Epoch 66/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.3642e-06 - val_loss: 6.8580e-05\n",
      "Epoch 67/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.2112e-06 - val_loss: 5.8870e-05\n",
      "Epoch 68/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 8.9976e-07 - val_loss: 5.8063e-05\n",
      "Epoch 69/120\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.2331e-06 - val_loss: 7.4065e-05\n",
      "Epoch 70/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.0321e-06 - val_loss: 6.1766e-05\n",
      "Epoch 71/120\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.0470e-06 - val_loss: 5.6225e-05\n",
      "Epoch 72/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 6.8451e-07 - val_loss: 5.7932e-05\n",
      "Epoch 73/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 6.7290e-07 - val_loss: 5.7042e-05\n",
      "Epoch 74/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.1690e-06 - val_loss: 5.2356e-05\n",
      "Epoch 75/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 8.1518e-07 - val_loss: 5.1889e-05\n",
      "Epoch 76/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 7.6354e-07 - val_loss: 6.2544e-05\n",
      "Epoch 77/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 7.9745e-07 - val_loss: 5.9349e-05\n",
      "Epoch 78/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 1.4181e-06 - val_loss: 7.6562e-05\n",
      "Epoch 79/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 7.0304e-07 - val_loss: 5.1399e-05\n",
      "Epoch 80/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 9.6188e-07 - val_loss: 5.1441e-05\n",
      "Epoch 81/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 7.1437e-07 - val_loss: 5.2547e-05\n",
      "Epoch 82/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 9.1138e-07 - val_loss: 5.0340e-05\n",
      "Epoch 83/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 6.8990e-07 - val_loss: 5.3233e-05\n",
      "Epoch 84/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 8.8179e-07 - val_loss: 5.7158e-05\n",
      "Epoch 85/120\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 5.5128e-07 - val_loss: 5.1208e-05\n",
      "Epoch 86/120\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 8.2513e-07 - val_loss: 5.4888e-05\n",
      "Epoch 87/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 5.5508e-07 - val_loss: 4.8309e-05\n",
      "Epoch 88/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.0491e-06 - val_loss: 5.2082e-05\n",
      "Epoch 89/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 7.0332e-07 - val_loss: 5.4267e-05\n",
      "Epoch 90/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 6.0386e-07 - val_loss: 4.9693e-05\n",
      "Epoch 91/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 7.7262e-07 - val_loss: 5.9267e-05\n",
      "Epoch 92/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 1.0555e-06 - val_loss: 4.9306e-05\n",
      "Epoch 93/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 6.3035e-07 - val_loss: 5.5984e-05\n",
      "Epoch 94/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 5.5204e-07 - val_loss: 4.9317e-05\n",
      "Epoch 95/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 8.3964e-07 - val_loss: 5.9042e-05\n",
      "Epoch 96/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 6.0962e-07 - val_loss: 4.9513e-05\n",
      "Epoch 97/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.1639e-06 - val_loss: 5.3236e-05\n",
      "Epoch 98/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 4.8553e-07 - val_loss: 4.9223e-05\n",
      "Epoch 99/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 3.6299e-07 - val_loss: 4.8227e-05\n",
      "Epoch 100/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 5.3310e-07 - val_loss: 7.2309e-05\n",
      "Epoch 101/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 4.1347e-07 - val_loss: 5.0755e-05\n",
      "Epoch 102/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 5.9560e-07 - val_loss: 4.9933e-05\n",
      "Epoch 103/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 9.4934e-07 - val_loss: 5.2587e-05\n",
      "Epoch 104/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 6.1925e-07 - val_loss: 4.9854e-05\n",
      "Epoch 105/120\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 5.8091e-07 - val_loss: 5.2695e-05\n",
      "Epoch 106/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 3.9781e-07 - val_loss: 4.9006e-05\n",
      "Epoch 107/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 4.5661e-07 - val_loss: 5.1922e-05\n",
      "Epoch 108/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 3.5180e-07 - val_loss: 5.5153e-05\n",
      "Epoch 109/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 5.5159e-07 - val_loss: 4.9070e-05\n",
      "Epoch 110/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 6.4440e-07 - val_loss: 5.6009e-05\n",
      "Epoch 111/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 4.9772e-07 - val_loss: 4.6259e-05\n",
      "Epoch 112/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 4.2267e-07 - val_loss: 4.8981e-05\n",
      "Epoch 113/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 6.0266e-07 - val_loss: 5.2266e-05\n",
      "Epoch 114/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 5.6498e-07 - val_loss: 4.6686e-05\n",
      "Epoch 115/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 4.5012e-07 - val_loss: 4.9021e-05\n",
      "Epoch 116/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 5.4493e-07 - val_loss: 4.8366e-05\n",
      "Epoch 117/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 5.1504e-07 - val_loss: 4.9632e-05\n",
      "Epoch 118/120\n",
      "125/125 [==============================] - 2s 13ms/step - loss: 4.8320e-07 - val_loss: 5.9379e-05\n",
      "Epoch 119/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 6.1809e-07 - val_loss: 5.2502e-05\n",
      "Epoch 120/120\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 4.6132e-07 - val_loss: 4.7182e-05\n",
      "22/22 [==============================] - 1s 8ms/step\n",
      "{0: ({'type_of_data': 'Price', 'add_return': False, 'add_log_functions': True, 'drop_higly_correlated_data': False, 'add_analysed_curves': False, 'scaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'feature_range': (0, 1), 'rnn_size': 1, 'size_train': 2000, 'size_valid': 800, 'target_id': 0, 'type_of_rnn': 'GRU', 'nb_layers': 1, 'nb_units': [256], 'learning_rate': 0.001, 'optimizer': <class 'keras.optimizers.optimizer_v2.adam.Adam'>, 'loss': <keras.losses.MeanSquaredError object at 0x7f82c2f76670>, 'batch_size': 16, 'epochs': 120}, 1059.0485235388353), 1: ({'type_of_data': 'Price', 'add_return': False, 'add_log_functions': False, 'drop_higly_correlated_data': False, 'add_analysed_curves': True, 'scaler': <class 'sklearn.preprocessing._data.MinMaxScaler'>, 'feature_range': (0, 1), 'rnn_size': 10, 'size_train': 2000, 'size_valid': 800, 'target_id': 0, 'type_of_rnn': 'GRU', 'nb_layers': 1, 'nb_units': [256], 'learning_rate': 0.001, 'optimizer': <class 'keras.optimizers.optimizer_v2.adam.Adam'>, 'loss': <keras.losses.MeanSquaredError object at 0x7f82c2f76670>, 'batch_size': 16, 'epochs': 120}, 472.9402464714686)}\n",
      "[1059.0485235388353, 472.9402464714686]\n"
     ]
    }
   ],
   "source": [
    "search_best_params = {}\n",
    "for i,par in enumerate(liste_params_chosen):\n",
    "    search_best_params[i] = (par,evaluate(par))\n",
    "print(search_best_params)\n",
    "l = []\n",
    "l.extend(search_best_params.values())\n",
    "# print(l[0])\n",
    "l1 = []\n",
    "for j in range(N):\n",
    "    l1.append(l[j][1])\n",
    "print(l1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
